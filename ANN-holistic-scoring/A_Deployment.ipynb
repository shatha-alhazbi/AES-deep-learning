{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Deployment Model"
      ],
      "metadata": {
        "id": "QLDX55LdA4O-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "igWuFNRPAhJm"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SCORE_RANGES = {\n",
        "        1: {'sentence_fluency': (1, 6), 'word_choice': (1, 6), 'conventions': (1, 6),'organization': (1, 6),\n",
        "            'content': (1, 6), 'holistic': (2, 12)},\n",
        "        2: {'sentence_fluency': (1, 6), 'word_choice': (1, 6), 'conventions': (1, 6),'organization': (1, 6),\n",
        "            'content': (1, 6), 'holistic': (1, 6)},\n",
        "        3: {'narrativity': (0, 3), 'language': (0, 3), 'prompt_adherence': (0, 3), 'content': (0, 3), 'holistic': (0, 3)},\n",
        "        4: {'narrativity': (0, 3), 'language': (0, 3), 'prompt_adherence': (0, 3), 'content': (0, 3), 'holistic': (0, 3)},\n",
        "        5: {'narrativity': (0, 4), 'language': (0, 4), 'prompt_adherence': (0, 4), 'content': (0, 4), 'holistic': (0, 4)},\n",
        "        6: {'narrativity': (0, 4), 'language': (0, 4), 'prompt_adherence': (0, 4), 'content': (0, 4), 'holistic': (0, 4)},\n",
        "        7: {'conventions': (0, 6), 'organization': (0, 6), 'content': (0, 6),'holistic': (0, 30)},\n",
        "        8: {'sentence_fluency': (2, 12), 'word_choice': (2, 12), 'conventions': (2, 12),'organization': (2, 12),\n",
        "            'content': (2, 12), 'holistic': (0, 60)}}\n",
        "\n",
        "def read_data(path):\n",
        "    \"\"\"Read the CSV file and return a dictionary of data\"\"\"\n",
        "    data = pd.read_csv(path)\n",
        "    data_dict = {\n",
        "        'essay_ids': data['essay_id'].values,\n",
        "        'prompt_ids': data['prompt_id'].values,\n",
        "        'essay_text': data['essay_text'].values,\n",
        "        'features': data.iloc[:, 12:].values,\n",
        "        'holistic': data['holistic'].values\n",
        "    }\n",
        "    return data_dict\n",
        "\n",
        "def quadratic_weighted_kappa(y_true, y_pred):\n",
        "    \"\"\"Calculate QWK score between true and predicted labels\"\"\"\n",
        "    return cohen_kappa_score(y_true, np.round(y_pred), weights='quadratic')\n"
      ],
      "metadata": {
        "id": "eDke-ZDpA8Dx"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_scores(scores, prompt_id):\n",
        "    score_range = SCORE_RANGES[prompt_id]['holistic']\n",
        "    return (scores - score_range[0]) / (score_range[1] - score_range[0])\n",
        "\n",
        "def denormalize_scores(norm_scores, prompt_id):\n",
        "    score_range = SCORE_RANGES[prompt_id]['holistic']\n",
        "    return norm_scores * (score_range[1] - score_range[0]) + score_range[0]\n"
      ],
      "metadata": {
        "id": "vmXaFkw-BVnS"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self, input_size, hidden_unit, num_layers):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        layers = []\n",
        "        layers.append(nn.Linear(input_size, hidden_unit))\n",
        "        layers.append(nn.ReLU())\n",
        "        for i in range(num_layers - 1):\n",
        "            layers.append(nn.Linear(hidden_unit, hidden_unit))\n",
        "            layers.append(nn.ReLU())\n",
        "        layers.append(nn.Linear(hidden_unit, 1))\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "        for i in self.modules():\n",
        "            if isinstance(i, nn.Linear):\n",
        "                nn.init.kaiming_normal_(i.weight)\n",
        "                nn.init.zeros_(i.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n"
      ],
      "metadata": {
        "id": "AfgLpjIqCJRM"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DeploymentModel(nn.Module):\n",
        "    def __init__(self, base_model):\n",
        "        super(DeploymentModel, self).__init__()\n",
        "        self.model = base_model\n",
        "\n",
        "    def forward(self, features, prompt_ids):\n",
        "        # Get normalized predictions\n",
        "        normalized_preds = self.model(features)\n",
        "\n",
        "        # Denormalize for each prompt_id\n",
        "        batch_size = features.shape[0]\n",
        "        denormalized_preds = torch.zeros_like(normalized_preds)\n",
        "\n",
        "        for i in range(batch_size):\n",
        "            denormalized_preds[i] = denormalize_scores(\n",
        "                normalized_preds[i],\n",
        "                prompt_ids[i].item()\n",
        "            )\n",
        "\n",
        "        return denormalized_preds"
      ],
      "metadata": {
        "id": "zRa-OBQCRd0h"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    print(f\"\\n{'-'*50}\")\n",
        "    print(f\"deployment model training started at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "    print(f\"{'-'*50}\\n\")\n",
        "\n",
        "    print(\"Reading dataset\\n\")\n",
        "    data = read_data('dataset.csv')\n",
        "\n",
        "    features = torch.FloatTensor(data['features'])\n",
        "    scores = torch.FloatTensor(data['holistic'])\n",
        "    prompt_ids = data['prompt_ids']\n",
        "\n",
        "    print(f\"Dataset loaded: {len(features)} essays, {features.shape[1]} features\")\n",
        "\n",
        "    normalized_scores = torch.zeros_like(scores)\n",
        "    for prompt_id in range(1, 9):\n",
        "        prompt_filter = prompt_ids == prompt_id\n",
        "        normalized_scores[prompt_filter] = normalize_scores(scores[prompt_filter], prompt_id)\n",
        "    normalized_scores = normalized_scores.reshape(-1, 1)\n",
        "\n",
        "    final_params = {\n",
        "        'hidden_unit': 32,\n",
        "        'num_layers': 2,\n",
        "        'learning_rate': 0.001,\n",
        "        'batch_size': 32\n",
        "    }\n",
        "\n",
        "    print(\"\\ninitializing model with best parameters:\")\n",
        "    for param, value in final_params.items():\n",
        "        print(f\"{param}: {value}\")\n",
        "\n",
        "    model = NeuralNetwork(\n",
        "        input_size=86,\n",
        "        hidden_unit=final_params['hidden_unit'],\n",
        "        num_layers=final_params['num_layers']\n",
        "    )\n",
        "\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=final_params['learning_rate'],\n",
        "        betas=(0.9, 0.999),\n",
        "        weight_decay=0.1\n",
        "    )\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    best_loss = float('inf')\n",
        "    epochs_no_improve = 0\n",
        "    batch_size = final_params['batch_size']\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(15):\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "        num_batches = 0\n",
        "\n",
        "        for i in range(0, len(features), batch_size):\n",
        "            batch_X = features[i:i + batch_size]\n",
        "            batch_y = normalized_scores[i:i + batch_size]\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(batch_X)\n",
        "            loss = criterion(outputs, batch_y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            num_batches += 1\n",
        "\n",
        "        avg_epoch_loss = epoch_loss / num_batches\n",
        "        print(f\"Epoch {epoch + 1}/15 - Loss: {avg_epoch_loss:.6f}\")\n",
        "\n",
        "        if avg_epoch_loss < best_loss:\n",
        "            best_loss = avg_epoch_loss\n",
        "            epochs_no_improve = 0\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "\n",
        "        if epochs_no_improve >= 3:\n",
        "            print(f\"\\nEarly stopping at epoch {epoch + 1}\")\n",
        "            break\n",
        "\n",
        "    training_time = time.time() - start_time\n",
        "\n",
        "    print(f\"\\n{'-'*50}\")\n",
        "    print(\"training completed successfully!  :)\")\n",
        "    print(f\"Training time: {training_time/60:.2f} minutes\")\n",
        "    print(f\"Final loss: {best_loss:.6f}\")\n",
        "    print(f\"{'-'*50}\\n\")\n",
        "\n",
        "    # Save the base model (outputs normalized scores)\n",
        "    scripted_model = torch.jit.script(model)\n",
        "    scripted_model.save(\"model-A-deployment.pt\")\n",
        "    print(\"\\nModel saved as 'model-A-deployment.pt'\")\n",
        "\n",
        "    #verify predictions with manual denormalization\n",
        "    print(\"\\nVerifying predictions:\")\n",
        "    with torch.no_grad():\n",
        "        test_batch = features[:5]\n",
        "        test_prompts = prompt_ids[:5]\n",
        "        normalized_preds = model(test_batch)\n",
        "\n",
        "        #manually denormalize\n",
        "        denormalized_preds = torch.zeros_like(normalized_preds)\n",
        "        for i in range(len(test_batch)):\n",
        "            denormalized_preds[i] = denormalize_scores(normalized_preds[i], test_prompts[i])\n",
        "\n",
        "        print(\"Sample predictions (denormalized):\", denormalized_preds.numpy().flatten())\n",
        "        print(\"Original scores:\", scores[:5])\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LdfmtC6ICw7N",
        "outputId": "df393544-70d7-40df-d35b-fdeef3fb668c"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--------------------------------------------------\n",
            "deployment model training started at 2024-12-07 13:29:30\n",
            "--------------------------------------------------\n",
            "\n",
            "Reading dataset\n",
            "\n",
            "Dataset loaded: 12976 essays, 86 features\n",
            "\n",
            "initializing model with best parameters:\n",
            "hidden_unit: 32\n",
            "num_layers: 2\n",
            "learning_rate: 0.001\n",
            "batch_size: 32\n",
            "Epoch 1/15 - Loss: 0.032144\n",
            "Epoch 2/15 - Loss: 0.022328\n",
            "Epoch 3/15 - Loss: 0.021579\n",
            "Epoch 4/15 - Loss: 0.021213\n",
            "Epoch 5/15 - Loss: 0.020940\n",
            "Epoch 6/15 - Loss: 0.020660\n",
            "Epoch 7/15 - Loss: 0.020448\n",
            "Epoch 8/15 - Loss: 0.020271\n",
            "Epoch 9/15 - Loss: 0.020153\n",
            "Epoch 10/15 - Loss: 0.020071\n",
            "Epoch 11/15 - Loss: 0.020016\n",
            "Epoch 12/15 - Loss: 0.019867\n",
            "Epoch 13/15 - Loss: 0.019806\n",
            "Epoch 14/15 - Loss: 0.019685\n",
            "Epoch 15/15 - Loss: 0.019574\n",
            "\n",
            "--------------------------------------------------\n",
            "training completed successfully!  :)\n",
            "Training time: 0.16 minutes\n",
            "Final loss: 0.019574\n",
            "--------------------------------------------------\n",
            "\n",
            "\n",
            "Model saved as 'model-A-deployment.pt'\n",
            "\n",
            "Verifying predictions:\n",
            "Sample predictions (denormalized): [7.9796667 8.408892  7.3331165 9.5269575 8.369284 ]\n",
            "Original scores: tensor([ 8.,  9.,  7., 10.,  8.])\n"
          ]
        }
      ]
    }
  ]
}